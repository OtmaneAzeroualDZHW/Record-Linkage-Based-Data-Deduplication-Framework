# A Record Linkage-Based Data Deduplication Framework with DataCleaner Extension

The source code (Transitive-Clustering.java) can be set up directly in the DataCleaner tool and used to detect duplicates in research data (such as publication data).

Only sophisticated methods and algorithms or the human eye can recognize duplicates in research data. But human capabilities are limited. This is especially true with a large number of data sets. Manual methods fail at the latest from 5000 data records. After that, mechanical processes are needed. Excel is a good idea, but even searching for duplicates with Excel quickly reaches its limits. Therefore, better methods are required. Here we would like to show in a very short form what a good procedure must achieve and what results can be expected. We used DataCleaner software, which is a powerful data quality tool that allows over hundreds of thousands of publication data to automatically find the duplicates quickly and easily. An extension was developed in the tool to analyze the publication data and filter out the duplicates. The framework developed as a solution is intended to facilitate research in the field of data quality of research data and is therefore mainly to be regarded as a tool for scientists. Since research in this area is not yet very advanced, the framework must be flexible enough to easily integrate changes and extensions. Despite all the required flexibility, it must also be taken into account that the finished framework must be able to process large amounts of data. The aim is to implement an adaptive and flexible framework based on the presented model. This phase model thus provides for six phases: 1) data preparation, 2) search space definition, 3) attribute value comparison, 4) decision model, 6) clustering of the duplicates and 6) verification. The implementation of this model was very time consuming and the most important phases have to be implemented. Particular emphasis is placed on the phases of comparing attribute values to the decision model and clustering duplicates. These phases perform the essential steps of duplicate detection. Other phases are trivial (search space definition) or not implemented (data preparation, verification). When implementing the attribute value comparison, it should be noted that when comparing two x-tuples, all alternatives must be compared with each other. When comparing the alternatives, all attributes and all versions of the attribute values must be compared. In addition, it must be possible to define several similarity functions for an attribute and to aggregate the resulting similarities (per attribute per similarity function) in a weighted manner.
When implementing the framework in the Data Cleaner Tool, there should be some similarity functions, but new ones must be able to be added if necessary. The similarity functions should be normalized, i.e. always return a value between 0 and 1, otherwise the probabilities can be falsified. The configuration took place on two levels: On the one hand, the components are assembled and made known, this happens before the program is loaded. The other part is configuration, which is created and processed at runtime. To assess the performance of the framework, the relevance of the results and the required runtime are considered. A high relevance in the duplicate detection results when the number of real duplicates found is as high as possible ("hit rate"), while as few data records as possible should be falsely recognized as duplicates ("accuracy"). In the scientific literature only these factors are known as "recall" and "precision". It is easy to see that both factors are interdependent and that the hit rate generally decreases with increasing accuracy. A developed framework that simply marks each record as a duplicate will actually "detect" all existing duplicates and would have a 100% hit rate. However, most of the results will be irrelevant and the precision will be correspondingly low. Another framework, which only marks duplicates as such if it is absolutely certain, will have a high level of accuracy, but may miss many duplicates and thus have a low hit rate.
Depending on the application, a high hit rate or high accuracy can be considered more important, which is why it makes sense to make this configurable for the user. In the framework presented here, this is made possible by setting the threshold value from which a data record is recognized as a duplicate of another data record.
The evaluation of the factors as "recall" and "precision" requires a suitable selection of test data. Ideally, this should be authentic data from practical applications. To consider relevance, it is also important that all duplicates that actually exist are known, because this is the only way to accurately calculate the hit rate and accuracy. In small databases, this can be achieved by manually reviewing all data sets. This is practically no longer possible in large databases. Since research data (e.g. project data, patent data, third-party funded data, etc.) is usually confidential data, it is difficult to access and not suitable for publication, which means that results cannot be reproduced later.
An alternative is self-generated data, the structure of which should reflect reality as well as possible. Since the duplicates are also generated in this case, it is known exactly which duplicates are in the database and the hit rate and accuracy can be determined exactly. This alternative is used here to evaluate the algorithm. The resulting solution was also tested on other databases containing real publication data to ensure that the results can also be transferred to practice. However, since it was not known in advance which duplicates were in the database, these can only offer a subjective impression of the relevance of the results.

Rather, the following paper can be found on the subject:

Azeroual, O.; Jha, M.; Nikiforova, A.; Sha, K.; Alsmirat, M.; Jha, S.  A Record Linkage-Based Data Deduplication Framework with DataCleaner Extension. Multimodal Technologies and Interaction, 2022, (forthcoming).






